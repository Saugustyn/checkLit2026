================================================================
CHECKLIT – PODSUMOWANIE ETAP 2: TESTOWANIE I KALIBRACJA
================================================================
Data: luty 2026
Stan: ✅ Backend przetestowany end-to-end | ⚠️ Frontend nie testowany

----------------------------------------------------------------
CO ZROBILIŚMY W ETAPIE 2
----------------------------------------------------------------

1. Pierwszy test end-to-end przez Swagger (localhost:8000/docs)
2. Wykryto i naprawiono 3 problemy z detekcją AI
3. Zamieniono wskaźnik czytelności Flesch → LIX
4. Skalibrowano progi perplexity dla polskich tekstów literackich

----------------------------------------------------------------
PROBLEM 1: sacremoses – brak paczki
----------------------------------------------------------------

Objaw:
  "confidence": "Niska (tryb heurystyczny – model niedostępny)"
  Herbert ładował się ale crashował przy tokenizacji

Przyczyna:
  HerbertTokenizer wymaga paczki sacremoses, której nie było
  w requirements.txt

Rozwiązanie:
  pip install sacremoses (w venv backendowym)
  Dodać sacremoses do requirements.txt na przyszłość

----------------------------------------------------------------
PROBLEM 2: Herbert (BERT) → złe wyniki detekcji AI
----------------------------------------------------------------

Objaw:
  Sienkiewicz (1896!) → 94% AI-generated
  Tekst napisany przez Claude → 94% AI-generated
  Oba teksty dostawały identyczny wynik – model nie rozróżniał

Przyczyna (ważna dla pracy inżynierskiej!):
  Herbert to model BERT (masked language model) – przewiduje
  ZAKRYTE tokeny w środku zdania. Używanie jego "loss" jako
  perplexity do detekcji AI jest BŁĘDEM metodologicznym.

  Do detekcji AI potrzebny jest model AUTOREGRESYWNY (causal LM)
  który przewiduje NASTĘPNY token. Tylko wtedy:
    niska perplexity = tekst "przewidywalny" = styl AI
    wysoka perplexity = tekst "zaskakujący" = styl ludzki

Rozwiązanie:
  Zamieniono model na sdadas/polish-gpt2-small
  (Polski GPT-2, ~500MB, autoregresywny, causal LM)

  Plik: backend/app/services/ai_detector.py

----------------------------------------------------------------
PROBLEM 3: Flesch Reading Ease – złe wyniki dla polskiego
----------------------------------------------------------------

Objaw:
  Sienkiewicz → flesch_score: 19.21 "Bardzo trudny"
  Tekst AI → flesch_score: 19.xx "Bardzo trudny"
  Każdy tekst wychodził jako "Bardzo trudny" bez różnicowania

Przyczyna:
  Flesch Reading Ease był kalibrowany pod język angielski.
  Opiera się na liczbie sylab – w polskim słowa są dłuższe
  i mają więcej sylab, co zawsze daje zaniżone wyniki.

Rozwiązanie:
  Zamieniono Flesch → LIX (Läsbarhetsindex)
  LIX jest językowo neutralny – używa długości słów (>6 znaków)
  zamiast sylab. Stosowany w badaniach nad językami słowiańskimi.

  Formuła: LIX = (słowa/zdania) + (słowa>6znaków * 100 / słowa)

  Skala LIX:
    < 25  → Bardzo łatwy (literatura dziecięca)
    25-35 → Łatwy (proza popularna)
    35-45 → Średni (beletrystyka)
    45-55 → Trudny (literatura poważna)
    > 55  → Bardzo trudny (proza awangardowa, teksty naukowe)

  Wynik po zmianie dla Sienkiewicza: 57.49 "Bardzo trudny /
  Proza awangardowa" – sensowny wynik dla XIX-wiecznej prozy!

  Plik: backend/app/services/nlp_service.py
  Plik: backend/app/schemas.py (dodano pola lix_*)

----------------------------------------------------------------
KALIBRACJA PROGÓW PERPLEXITY
----------------------------------------------------------------

Po zmianie na Polish GPT-2 wykonano testy porównawcze:

  Tekst AI (napisany przez Claude):   perplexity = 14.22
  Tekst ludzki (Sienkiewicz, 1896):   perplexity = 48.02

Wnioski:
  - Różnica wyraźna (14 vs 48) – model rozróżnia style ✅
  - Pierwotne progi (50/150) były zbyt wysokie

Nowe progi (zaktualizowane w ai_detector.py):
  PERPLEXITY_AI_THRESHOLD    = 25   (poniżej → AI)
  PERPLEXITY_HUMAN_THRESHOLD = 40   (powyżej → Human)

  14.22 < 25 → AI-generated ✅
  48.02 > 40 → Human-written ✅

UWAGA DO PRACY:
  Progi skalibrowane na 2 próbkach – to wstępna kalibracja.
  W rozdziale testów opisać jako wymagające walidacji na
  większym korpusie (min. 20-30 tekstów ludzkich + 20-30 AI).
  Jest to uczciwe i metodologicznie poprawne podejście.

----------------------------------------------------------------
AKTUALNY STAN PROJEKTU
----------------------------------------------------------------

  ✅ [1]  Setup projektu
  ✅ [2]  NLP + stylometria – przetestowane, działają
  ✅ [3]  Detekcja AI (Polish GPT-2 + perplexity) – działa
  ✅ [4]  Endpointy API – przetestowane przez Swagger
  ⚠️ [5]  /analyze + /results – kod gotowy, NIE testowane z UI
  ⚠️ [6]  /history – kod gotowy, NIE testowane z UI
  ⚠️ [7]  /compare – kod gotowy, NIE testowane z UI
  ❌ [8]  Testy pytest – NIE uruchomione
  ⚠️ [9]  Landing + About – kod gotowy, NIE widziane wizualnie
  ❌ [10] Eksport raportu – NIE zaimplementowany

----------------------------------------------------------------
AKTUALNE PLIKI (zmodyfikowane w etapie 2)
----------------------------------------------------------------

  backend/app/services/ai_detector.py   ← Polski GPT-2, nowe progi
  backend/app/services/nlp_service.py   ← LIX zamiast Flesch
  backend/app/schemas.py                ← Pola lix_*, perplexity

----------------------------------------------------------------
WYNIKI PRZYKŁADOWYCH ANALIZ
----------------------------------------------------------------

Tekst AI (Claude, fikcja):
  perplexity:     14.22
  ai_probability: 0.97
  label:          AI-generated ✅
  ttr:            0.9474
  lix_score:      50.82 "Literatura poważna"

Tekst ludzki (Sienkiewicz, Quo Vadis 1896):
  perplexity:     48.02
  ai_probability: 0.15 (po kalibracji) → Human-written ✅
  ttr:            0.8447
  lix_score:      57.49 "Proza awangardowa/teksty naukowe"

----------------------------------------------------------------
CO DALEJ (ETAP 3)
----------------------------------------------------------------

  → Test frontendu end-to-end (localhost:5173)
  → Uruchomienie pytest
  → Implementacja eksportu raportu (JSON/PDF)
  → Kalibracja progów na większym zbiorze tekstów
  → Aktualizacja requirements.txt (dodać sacremoses)

================================================================
