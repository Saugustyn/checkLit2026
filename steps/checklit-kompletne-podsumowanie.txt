================================================================
CHECKLIT – KOMPLETNE PODSUMOWANIE PROJEKTU (ETAPY 1–7)
================================================================
Data: luty 2026
Platforma: Automatyczna analiza autentyczności i stylu tekstów
           literackich — praca inżynierska
================================================================

----------------------------------------------------------------
SPIS TREŚCI
----------------------------------------------------------------

  1. Cel i zakres projektu
  2. Architektura systemu
  3. Przebieg prac (etap po etapie)
     3.1 Etap 1 — Setup i fundamenty
     3.2 Etap 2 — Pierwsze testy, Herbert → GPT-2, Flesch → LIX
     3.3 Etap 3 — Ewaluacja v1 (AUC=0.79, korpus zanieczyszczony)
     3.4 Etap 4 — Rekalibracja v2 (AUC=0.94, czysty korpus)
     3.5 Etap 5 — Pytest, MATTR, gęstość leksykalna
     3.6 Etap 6 — Ewaluacja live, rekalibracja v3 (AUC=0.90)
     3.7 Etap 7 — Redesign UI, sigmoida, spójność frontendu
  4. Ewolucja modelu detekcji AI (oś czasu)
  5. Metryki stylometryczne — co zmienialiśmy i dlaczego
  6. Analiza błędów klasyfikacji (materiał do pracy)
  7. Stan techniczny — pliki i technologie
  8. Status projektu (checklist)
  9. Co dalej (etap 8)
  10. Bibliografia

================================================================
1. CEL I ZAKRES PROJEKTU
================================================================

checkLit to platforma do automatycznej analizy autentyczności
i stylu tekstów literackich. Łączy detekcję AI (NLP/ML) ze
stylometrią ilościową.

GŁÓWNE FUNKCJONALNOŚCI:
  - Detekcja AI: klasyfikacja tekstu jako ludzkiego lub
    generowanego przez model językowy (z prawdopodobieństwem)
  - Stylometria: TTR/MATTR, entropia Shannona, gęstość
    leksykalna, bogactwo słownikowe, n-gramy
  - Jakość językowa: LIX (Läsbarhetsindex), gęstość
    interpunkcji, długość zdań i słów
  - Porównanie: zestawienie profili stylometrycznych dwóch
    tekstów z miarą podobieństwa
  - Historia analiz + eksport (JSON, PDF, TXT)
  - Upload pliku: .txt, .pdf, .docx (do 10MB)

DOCELOWI UŻYTKOWNICY:
  Wydawnictwa, instytucje naukowe, edukacyjne — w celu ochrony
  treści autorskich i weryfikacji autentyczności tekstów.

================================================================
2. ARCHITEKTURA SYSTEMU
================================================================

checkLit/
├── backend/
│   ├── app/
│   │   ├── main.py              # FastAPI + CORS + routing
│   │   ├── database.py          # SQLite + SQLAlchemy
│   │   ├── models.py            # Model Analysis (tabela: analyses)
│   │   ├── schemas.py           # Pydantic request/response
│   │   ├── routers/
│   │   │   └── analysis.py      # 5 endpointów REST
│   │   └── services/
│   │       ├── stylometry.py    # MATTR, entropia, n-gramy, lex.
│   │       ├── nlp_service.py   # LIX, długość słów, interpunkcja
│   │       ├── ai_detector.py   # GPT-2 + perplexity + sigmoida
│   │       └── file_parser.py   # Ekstrakcja z .txt/.pdf/.docx
│   ├── tests/
│   │   └── test_checklit.py     # 88 testów pytest
│   └── requirements.txt
├── frontend/
│   ├── src/
│   │   ├── App.jsx
│   │   ├── api/axios.js
│   │   ├── components/
│   │   │   └── Navbar.jsx       # SVG logo + nawigacja
│   │   └── pages/
│   │       ├── Home.jsx
│   │       ├── Analyze.jsx
│   │       ├── Results.jsx
│   │       ├── History.jsx
│   │       ├── Compare.jsx
│   │       └── About.jsx
│   ├── tailwind.config.js       # Paleta forest green
│   └── vite.config.js
├── evaluate.py                  # Ewaluacja korpusu (ROC, metryki)
├── evaluate_live.py             # Ewaluacja przez API (end-to-end)
├── make_corpus.py               # Budowanie/rozszerzanie korpusu
├── corpus_full.csv              # 80 tekstów (v3)
└── start.ps1                    # PowerShell: backend + frontend

ENDPOINTY API:
  POST   /api/analyze          → Analiza tekstu (JSON)
  POST   /api/analyze-file     → Analiza pliku (multipart/form-data)
  GET    /api/results/{id}     → Wyniki po ID
  GET    /api/history          → Lista analiz
  DELETE /api/history/{id}     → Usuń analizę
  POST   /api/compare          → Porównanie dwóch tekstów

STACK:
  Backend:  Python 3.11, FastAPI, SQLite, SQLAlchemy,
            Transformers 4.44.2, PyTorch 2.4.1,
            scikit-learn, scipy, pypdf, python-docx, Pytest
  Frontend: React 18, Vite, Tailwind CSS, Recharts, Axios,
            React Router DOM

================================================================
3. PRZEBIEG PRAC
================================================================

────────────────────────────────────────────────────────────────
3.1 ETAP 1 — SETUP I FUNDAMENTY
────────────────────────────────────────────────────────────────

Zbudowano pełną strukturę projektu od zera. Główne decyzje
architektoniczne podjęte na tym etapie:

PYTHON 3.11 zamiast 3.13/3.14:
  PyTorch 2.x nie ma wheel'i dla najnowszych wersji Pythona.
  Próby z 3.13 kończyły się błędami kompilacji C++.
  3.11 to "złoty standard" dla projektów ML/AI na Windows.

SQLITE zamiast PostgreSQL:
  Projekt akademicki, brak wymagań skalowalności.
  Zero konfiguracji, baza jako plik .db.

FASTAPI zamiast Flask/Django:
  Auto-generuje Swagger pod /docs — przydatne podczas testów
  i do dokumentacji pracy. Typowanie przez Pydantic.

MODEL DETEKCJI AI — pierwotna decyzja:
  Planowano allegro/herbert-base-cased + perplexity.
  Uzasadnienie: polski BERT od Allegro, metoda perplexity
  niezależna od konkretnego modelu generującego.

PROBLEMY NAPOTKANE:
  - spaCy/NLTK nie instalują się na Windows bez Build Tools C++
    → usunięto z requirements.txt, zastąpiono stdlib Python
  - Konflikt Python Launcher (legacy) z nową instalacją
    → odinstalowanie Python Launcher z Installed Apps

────────────────────────────────────────────────────────────────
3.2 ETAP 2 — PIERWSZE TESTY, DWIE FUNDAMENTALNE ZMIANY
────────────────────────────────────────────────────────────────

Pierwszy test end-to-end przez Swagger ujawnił trzy problemy,
z których dwa były fundamentalne metodologicznie.

PROBLEM 1 — sacremoses (techniczny):
  HerbertTokenizer crashował bez paczki sacremoses.
  Rozwiązanie: pip install sacremoses, dodać do requirements.txt.

PROBLEM 2 — Herbert (BERT) jako detektor AI [METODOLOGICZNY]:

  Objaw: Sienkiewicz (1896!) → 94% AI, tekst Claude → 94% AI.
  Oba teksty dostawały identyczny wynik.

  Przyczyna (ważna dla pracy inżynierskiej):
  Herbert to masked language model (BERT) — przewiduje ZAKRYTE
  tokeny w środku zdania. Używanie jego "loss" jako perplexity
  jest błędem metodologicznym.

  Do detekcji AI potrzebny jest model AUTOREGRESYWNY (causal LM),
  który przewiduje NASTĘPNY token. Tylko wtedy zachodzi zależność:
    niska perplexity = tekst przewidywalny = styl AI
    wysoka perplexity = tekst nieprzewidywalny = styl ludzki

  Rozwiązanie: zamiana na sdadas/polish-gpt2-small
  (Polski GPT-2, ~500MB, autoregresywny, causal LM)

  WNIOSEK DO PRACY: To fundamentalna różnica między architekturami
  encoder (BERT) i decoder (GPT). Warto ją opisać w rozdziale
  metodologicznym — pokazuje świadomość metodologiczną autora.

PROBLEM 3 — Flesch Reading Ease [JĘZYKOWY]:

  Objaw: każdy tekst wychodził jako "Bardzo trudny" niezależnie
  od rzeczywistej trudności. Sienkiewicz: 19.21, tekst AI: 19.xx.

  Przyczyna: Flesch był kalibrowany pod angielski. Opiera się na
  liczbie sylab — w polskim słowa są dłuższe i mają więcej sylab,
  co mechanicznie obniża wynik dla każdego tekstu polskiego.

  Rozwiązanie: zamiana na LIX (Läsbarhetsindex):
    LIX = (słowa/zdania) + (słowa>6znaków × 100 / słowa)
    Językowo neutralny — mierzy długość słów, nie sylaby.
    Stosowany w badaniach nad językami słowiańskimi.

  Po zmianie: Sienkiewicz → 57.49 "Bardzo trudny / Proza
  awangardowa" — sensowny wynik dla XIX-wiecznej prozy.

────────────────────────────────────────────────────────────────
3.3 ETAP 3 — EWALUACJA v1 (KORPUS ZANIECZYSZCZONY)
────────────────────────────────────────────────────────────────

Przeprowadzono pierwszą formalną ewaluację. Korpus v1: 25 human
+ 25 AI = 50 tekstów.

WYNIKI v1:
  AUC-ROC:    0.7936
  Accuracy:   80%
  Precision:  72.7%
  Recall:     96%
  F1:         82.8%

Recall = 0.96 — system wykrył 24/25 tekstów AI. Dla detektora
jest to priorytet (lepiej fałszywy alarm niż przeoczenie AI).

Optymalny próg (Youden): 38.45

PROBLEM ODKRYTY PÓŹNIEJ:
  15 z 25 tekstów w klasie "human" było wygenerowanych przez AI
  (proza oryginalna pisana na potrzeby testu). Ewaluacja mierzyła
  de facto: AI vs AI-udające-human. Wyniki v1 były artefaktem
  złego korpusu. → Etap 4: pełna wymiana korpusu.

DODANE FUNKCJONALNOŚCI:
  - file_parser.py: ekstrakcja z .txt, .pdf, .docx
  - POST /api/analyze-file: nowy endpoint (multipart/form-data)
  - Analyze.jsx: tryb upload + drag & drop
  - start.ps1: skrypt PowerShell (backend + frontend)

────────────────────────────────────────────────────────────────
3.4 ETAP 4 — REKALIBRACJA v2 (CZYSTY KORPUS, AUC=0.94)
────────────────────────────────────────────────────────────────

Zastąpiono 100% tekstów klasy "human" weryfikowalnymi źródłami
z domeny publicznej (Wolne Lektury, wolnelektury.pl).

KORPUS v2 — klasa "human" (25 tekstów, 7 autorów):
  Prus (Lalka t.1/t.2, Faraon t.1/t.2, Placówka)
  Żeromski (Ludzie bezdomni t.1/t.2, Przedwiośnie)
  Sienkiewicz (Quo Vadis, Ogniem i mieczem, Potop t.1/2/3,
               Listy z Ameryki)
  Reymont (Chłopi — Zima, Jesień)
  Orzeszkowa (W klatce, Gloria victis)
  Konopnicka (Wrażenia z podróży)
  Przybyszewski (Moi współcześni)

WYNIKI v2:
  AUC-ROC:    0.9376   (+0.144 vs v1)
  Accuracy:   90%      (+10%)
  Precision:  88.5%    (+16%)
  Recall:     92%      (-4%, akceptowalne)
  F1:         90.2%    (+8%)

AUC > 0.90 = "doskonała dyskryminacja" (Hosmer & Lemeshow, 2000)

Nowe progi (ROC/Youden):
  PERPLEXITY_AI_THRESHOLD    = 28.89
  PERPLEXITY_HUMAN_THRESHOLD = 37.04

ANALIZA BŁĘDÓW v2 (5 tekstów):
  Fałszywe alarmy (3):
    Prus — Wokulski w pociągu          ppx=28.26
    Reymont — Boryna na miedzy         ppx=29.19
    Sienkiewicz — Soroka i Kmicic      ppx=36.46
  Przeoczenia AI (2):
    ppx=47.35 — nieregularna składnia
    ppx=37.04 — emocjonalne pauzy

  Wzorzec: system myli się gdy ludzki tekst ma prostą narrację
  epicką lub tekst AI ma nieregularną, emocjonalną składnię.
  → Perplexity mierzy "regularność" stylu, nie "człowieczeństwo".

────────────────────────────────────────────────────────────────
3.5 ETAP 5 — PYTEST, MATTR, GĘSTOŚĆ LEKSYKALNA
────────────────────────────────────────────────────────────────

NAPRAWA METRYK STYLOMETRYCZNYCH:

Problem 1: TTR i lexical_density zwracały identyczną wartość.
  calculate_lexical_density() dosłownie wywoływała calculate_ttr().
  Dwie różne karty na frontendzie pokazywały tę samą liczbę.

Problem 2: TTR zależny od długości tekstu.
  Krótki tekst (~100 tokenów): TTR ~0.85-0.95
  Długi tekst (1000+ tokenów): TTR ~0.40-0.50
  Metryka nieporównywalna między tekstami — znany problem
  w literaturze stylometrycznej.

ROZWIĄZANIE — MATTR (Moving Average Type-Token Ratio):
  Okno 50 tokenów, uśrednione po całym tekście.
  Standard w stylometrii obliczeniowej (Covington & McFall, 2010).
  Stabilny dla tekstów różnej długości.

ROZWIĄZANIE — Lexical Density:
  Stosunek słów treściowych (nie-stopwords) do wszystkich tokenów.
  50 polskich stopwords (i, w, z, na, do, się, nie, to...).
  Interpretacja:
    >0.7: tekst merytoryczny (dużo rzeczowników/czasowników)
    <0.5: tekst narracyjny (dużo słów funkcyjnych)

TESTY PYTEST — 88 testów:

  Sekcja            | Testów | Passed | Failed
  ──────────────────|--------|--------|────────
  Stylometria       |   48   |   48   |    0
  File Parser       |    8   |    6   |    2
  AI Detector       |   12   |   12   |    0
  API FastAPI       |   20   |   20   |    0
  TOTAL             |   88   |   86   |    2   (97.7%)

  Czas: ~1.57s (mock modelu GPT-2 — bez ładowania modelu)

  2 failed to edge cases file parsera (za krótkie teksty testowe,
  encoding cp1250) — redundantne, file upload działa poprawnie
  w testach integracyjnych API.

NAPRAWIONE BŁĘDY PODCZAS PISANIA TESTÓW:
  - extract_text: zła kolejność argumentów (filename, content)
  - API: backend zwraca 400 dla krótkiego tekstu, nie 422
  - Compare: API wymaga text_a/text_b, nie text1/text2
  - MATTR: zbyt optymistyczny assert stabilności

────────────────────────────────────────────────────────────────
3.6 ETAP 6 — EWALUACJA LIVE, REKALIBRACJA v3 (AUC=0.90)
────────────────────────────────────────────────────────────────

PROBLEM — PROGI POZA ZAKRESEM:
  Nowy skrypt evaluate_live.py wysłał 30 tekstów do API.
  Wszystkie 30 dostały ai_probability = 0.03 (Human-written).

  Przyczyna: teksty AI w nowej partii były pisane starannym
  stylem literackim (ppx 99–271), podczas gdy korpus v2 zawierał
  głównie formulaiczne generacje (ppx ~14–35).

  Rozkład perplexity (30 nowych tekstów):
    AI:    średnia 192.7, mediana 184.8, zakres 99.6–270.9
    Human: średnia 310.4, mediana 258.9, zakres 171.0–677.0
    Nakładanie zakresów: 171–271 (duże!)

  Wniosek: metoda perplexity zawodzi na AI pisanym stylem
  literackim. Jest to znane ograniczenie opisane w literaturze
  (Gehrmann et al., 2019; Ippolito et al., 2019).

REKALIBRACJA v3 — korpus 80 tekstów (corpus_full.csv):
  Dodano 30 nowych tekstów przez make_corpus.py.

WYNIKI v3:
  AUC-ROC:    0.9029   (vs v2: 0.9376)
  Accuracy:   82.5%    (vs v2: 90%)
  Precision:  76.1%    (vs v2: 88.5%)
  Recall:     92.1%    (vs v2: 92%) ← stabilny!
  F1:         83.3%    (vs v2: 90.2%)

Nowe progi:
  PERPLEXITY_AI_THRESHOLD    = 32.03
  PERPLEXITY_HUMAN_THRESHOLD = 41.0623
  Strefa szara: 32.03 – 41.06

INTERPRETACJA WYNIKÓW — WNIOSEK BADAWCZY:

  Spadek precision (88% → 76%) to naturalny efekt
  trudniejszego korpusu — więcej fałszywych alarmów na tekstach
  ludzkich o prostej narracji i mniej formulaicznych tekstów AI.

  Recall pozostał stabilny (92%) — system nadal wykrywa
  większość tekstów AI niezależnie od stylu.

  AUC = 0.90 na trudniejszym korpusie potwierdza użyteczność
  systemu. Porównanie scenariuszy:

  Scenariusz      | n  | AUC  | Accuracy | Precision | Recall
  ──────────────────────────────────────────────────────────
  v2 formulaiczny | 50 | 0.94 |   90%    |   88%     |  92%
  v3 literacki    | 80 | 0.90 |  82.5%   |   76%     |  92%

────────────────────────────────────────────────────────────────
3.7 ETAP 7 — REDESIGN UI, SIGMOIDA, SPÓJNOŚĆ FRONTENDU
────────────────────────────────────────────────────────────────

NAPRAWA WYNIKÓW AI (3% / 97%):

  Problem: historia analiz pokazywała wyłącznie 3% lub 97%.
  Diagnoza: stara funkcja piecewise linear miała twarde podłogi:
    max(0.03, ...) i min(0.97, ...)
  Teksty polskiej literatury klasycznej mają perplexity
  znacznie powyżej 41.06 → zawsze 3%. Teksty AI poniżej 32.03
  → zawsze 97%.

  Rozwiązanie — Sigmoida:
    ai_prob = 1.0 / (1.0 + exp(k × (perplexity - midpoint)))
    midpoint = 36.5, k = 0.15

  Efekt: ciągły rozkład (ppx=30→74%, ppx=37→50%, ppx=45→23%)

AKTUALIZACJA TREŚCI (spójność z v3):
  Results.jsx: 6 miejsc — AUC=0.94/n=50 → AUC=0.90/n=80,
               progi 28.9–37.0 → 32.03–41.06,
               segmenty ScaleBar 40/20/40 → 32/9/59
  About.jsx:   model, rekalibracja, stack
  History.jsx: Flesch → LIX, progi aiLabel → 0.32/0.41,
               fallback a.lix_score ?? a.flesch_score
  Home.jsx:    "Flesch" → "LIX"

REDESIGN UI:
  tailwind.config.js: paleta "forest green" (#2d6a4f)
    Poprzednia: niebieski #4f6ef7
    Nowa: primary-500 #2d6a4f, primary-600 #1f5240

  Navbar.jsx: nowe SVG logo (monogram: linie tekstu + strzałka
  check na zielonym tle), wordmark "checkLit"

  Wszystkie 6 komponentów: emoji → inline SVG icons
  AlertBox: info kolor blue → primary green
  Compare.jsx: niebieski → green, orange → amber,
               radar chart #2d6a4f + #d97706

================================================================
4. EWOLUCJA MODELU DETEKCJI AI
================================================================

  Wersja  | Model            | Metoda       | n  | AUC  | Progi AI/Human
  ────────────────────────────────────────────────────────────────────────
  Etap 1  | Herbert (BERT)   | Masked LM    | — | n/d  | 60/120 (zgadnięte)
  v1      | Polish GPT-2     | Causal LM    | 50 | 0.79 | 25/40 (zgadnięte)
  v1.1    | Polish GPT-2     | Causal LM    | 50 | 0.79 | 30/38.45 (ROC)
  v2      | Polish GPT-2     | Causal LM    | 50 | 0.94 | 28.89/37.04 (ROC)
  v3      | Polish GPT-2     | Causal LM    | 80 | 0.90 | 32.03/41.06 (ROC)

KLUCZOWE ZMIANY:
  Herbert → GPT-2: poprawka błędu metodologicznego
                   (encoder vs decoder architecture)
  Zgadnięte → ROC/Youden: matematycznie uzasadnione progi
  n=50 → n=80: bardziej reprezentatywny korpus
  Piecewise → Sigmoida: ciągły rozkład prawdopodobieństwa

================================================================
5. METRYKI STYLOMETRYCZNE — EWOLUCJA
================================================================

  Metryka         | Wersja 1           | Wersja finalna
  ────────────────────────────────────────────────────────────
  Czytelność      | Flesch Reading Ease | LIX (Läsbarhetsindex)
  TTR             | Raw TTR (zależny    | MATTR okno 50 tokenów
                  | od długości)        | (Covington & McFall, 2010)
  Lexical Density | = TTR (duplikat!)  | % słów treściowych (50 stop.)
  Entropia        | Shannon            | Shannon (bez zmian)
  N-gramy         | Bigramy            | Bigramy count≥2 (bez zmian)
  Bogactwo słown. | Hapax legomena     | Hapax legomena (bez zmian)

UZASADNIENIA ZMIAN:
  Flesch → LIX: Flesch kalibrowany pod angielski (sylaby).
    LIX neutralny językowo (długość słów). Wyniki Flesch dla
    polskiego były systematycznie zaniżone niezależnie od tekstu.

  Raw TTR → MATTR: Raw TTR jest statystycznie zależny od długości
    tekstu — krótki tekst automatycznie ma wysokie TTR. MATTR
    eliminuje tę zależność przez okno przesuwne.

  Lexical Density: była dosłownym duplikatem TTR (bug w kodzie).
    Zamieniona na faktyczną metrykę: udział słów treściowych
    (nie-stopwords) wśród wszystkich tokenów.

================================================================
6. ANALIZA BŁĘDÓW KLASYFIKACJI (MATERIAŁ DO PRACY)
================================================================

FAŁSZYWE ALARMY — ludzkie teksty błędnie wykryte jako AI:

  Tekst                                      | ppx    | Przyczyna
  ──────────────────────────────────────────────────────────────
  Prus — Wokulski w pociągu (Lalka t.2)      | 28.26  | Introspekcja, krótkie zdania
  Reymont — Boryna na miedzy (Chłopi)        | 29.19  | Epika, prosta narracja 3os.
  Sienkiewicz — Soroka i Kmicic (Potop t.2)  | 36.46  | Dynamiczna akcja, blisko progu
  Zapolska — Dulska                          | 27.29  | Dialog narracyjny
  Orzeszkowa — Justyna (Nad Niemnem)         | 29.21  | Spokojny opis
  Orzeszkowa — Tadeusz (Nad Niemnem)         | 34.50  | Jw.
  Sienkiewicz — W pustyni i w puszczy        | 39.19  | Prosta przygodowa narracja
  Prus — Faraon t.1 Wstęp                    | 38.27  | Historyczna proza opisowa
  Reymont — Ziemia obiecana (Bauer)          | 37.06  | Tuż przy progu
  Mickiewicz — Ksiądz Robak                  | 37.53  | Jw.
  Proza ludowa — Babka Agnieszka             | 35.10  | Prosta narracja ludowa

PRZEOCZENIA — teksty AI niewykryte:

  Tekst                                      | ppx    | Przyczyna
  ──────────────────────────────────────────────────────────────
  Claude — pisarka przy biurku (Anna)        | 45.33  | Metaforyczny styl
  Claude — kamienica przy Złotej             | 41.06  | Emocjonalna składnia
  Claude — opis rzeczki                      | 40.91  | Poetycki, nieregularny
  Claude — [v2, nieznany]                    | 47.35  | Złożone zdania wieloczłonowe
  Claude — [v2, nieznany]                    | 37.04  | Tuż przy progu (dokładnie!)

WNIOSEK METODOLOGICZNY (do rozdziału 5/6 pracy):
  Perplexity mierzy "regularność" i "przewidywalność" stylu,
  a nie "człowieczeństwo" tekstu. Metoda zawodzi gdy:
    a) ludzki autor pisze regularnie i prosto (proza epická, ludowa)
    b) AI naśladuje nieregularny, emocjonalny styl ludzki

  Jest to fundamentalne ograniczenie klasy metod opartych na
  perplexity, opisane w literaturze naukowej:
  - Gehrmann et al. (2019): GLTR — pokazuje limity statystycznych
    metod detekcji
  - Ippolito et al. (2019): im bardziej zaawansowany model AI,
    tym trudniejsza detekcja metodą perplexity

  Rzetelne podejście: system podaje prawdopodobieństwo (nie wyrok),
  informuje o strefie szarej i ograniczeniach w UI.

================================================================
7. STAN TECHNICZNY
================================================================

BAZA DANYCH (models.py):
  Tabela: analyses
  Pola: id, created_at, text_preview, full_text, text_length,
        ai_probability, ttr, avg_sentence_length, lexical_density,
        entropy, flesch_score (←przechowuje LIX, nazwa historyczna),
        vocab_richness, full_results (JSON)

  UWAGA: kolumna nadal nazywa się flesch_score dla kompatybilności.
  Frontend używa: a.lix_score ?? a.flesch_score jako fallback.

URUCHOMIENIE:
  Terminal 1 (backend):
    cd checkLit/backend
    venv\Scripts\activate
    uvicorn app.main:app --reload
    → http://localhost:8000/docs

  Terminal 2 (frontend):
    cd checkLit/frontend
    npm run dev
    → http://localhost:5173

  Lub: ./start.ps1 (PowerShell, otwiera oba okna)

================================================================
8. STATUS PROJEKTU (CHECKLIST)
================================================================

  ✅ Setup środowiska (Python 3.11, Node.js, venv)
  ✅ FastAPI + SQLite — architektura backendu
  ✅ Stylometria: MATTR, entropia, gęstość lex., n-gramy, hapax
  ✅ Jakość językowa: LIX, długość zdań/słów, interpunkcja
  ✅ Detekcja AI: Polish GPT-2 + perplexity + sigmoida
  ✅ Kalibracja v1 (n=50, AUC=0.79, corpus zanieczyszczony)
  ✅ Rekalibracja v2 (n=50, AUC=0.94, Wolne Lektury)
  ✅ Rekalibracja v3 (n=80, AUC=0.90, AI literacki)
  ✅ API — 5 endpointów (analyze, analyze-file, results,
                         history, compare)
  ✅ Upload pliku (.txt, .pdf, .docx, do 10MB)
  ✅ Eksport raportu (JSON + PDF print)
  ✅ Ewaluacja automatyczna (evaluate.py + evaluate_live.py)
  ✅ Pytest — 86/88 passed (97.7%), mock GPT-2
  ✅ Frontend — redesign forest green + SVG icons
  ✅ Navbar — customowe SVG logo
  ✅ Sigmoida — ciągły rozkład prawdopodobieństwa
  ✅ Spójność frontendu z v3 (progi, AUC, n, LIX)
  ⚠️ Frontend — nie testowany end-to-end po redesignie
  ❌ Testy frontendu (Vitest) — kod gotowy, nie uruchomiony
  ❌ Pisanie pracy (rozdział 4.3, 5.1, 5.2, 6)
  ❌ Deployment (Docker) — opcjonalny

================================================================
9. CO DALEJ (ETAP 8)
================================================================

PRIORYTET 1 — Test end-to-end po redesignie:
  Uruchomić przez start.ps1, sprawdzić wizualnie:
  - Czy kolory forest green wyświetlają się poprawnie
  - Czy SVG ikony renderują się we wszystkich komponentach
  - Czy nowe progi (32/41) wyświetlają się w Results.jsx
  - Czy sigmoida daje różne wartości (nie tylko 3%/97%)
  - Czy kolumna LIX w History.jsx pokazuje wartości

PRIORYTET 2 — Pisanie pracy inżynierskiej:
  Rozdział 4.3: Implementacja detekcji AI
    - Metodologia (causal LM vs masked LM)
    - Opis sigmoidy i uzasadnienie parametrów
    - Opis stref (human/szara/AI) i progi ROC/Youden
  Rozdział 5.1: Ewaluacja — tabela porównawcza v1/v2/v3
  Rozdział 5.2: Analiza błędów klasyfikacji (tabela z sekcji 6)
  Rozdział 6: Wnioski i ograniczenia metody

MATERIAŁY DO PRACY JUŻ GOTOWE:
  → roc_curve.png (AUC=0.90)
  → evaluation_summary.json
  → Tabela porównawcza v1/v2/v3 (sekcja 4 tej notatki)
  → Tabela błędów klasyfikacji (sekcja 6)
  → Opis metodologiczny sigmoidy (etap 7)
  → Uzasadnienia zmian metryk (sekcja 5)

PRIORYTET 3 (opcjonalny) — Testy frontendu (Vitest):
  Kod testów checklit.test.jsx gotowy (102 testy).
  Nie jest wymagany do obrony pracy.

================================================================
10. BIBLIOGRAFIA
================================================================

  Covington, M. A., & McFall, J. D. (2010). Cutting the Gordian
    knot: The moving-average type–token ratio (MATTR).
    Journal of Quantitative Linguistics, 17(2), 94–100.

  Gehrmann, S., Strobelt, H., & Rush, A. M. (2019).
    GLTR: Statistical Detection and Visualization of Generated Text.
    ACL 2019. https://arxiv.org/abs/1906.04043

  Hosmer, D. W., & Lemeshow, S. (2000).
    Applied Logistic Regression (2nd ed.). Wiley.
    [klasyfikacja AUC: 0.7–0.8 akceptowalna, >0.9 doskonała]

  Ippolito, D., Duckworth, D., Callison-Burch, C., & Eck, D.
    (2019). Automatic Detection of Generated Text is Easiest when
    Humans are Fooled. arXiv:1911.00650.

  Ure, J. (1971). Lexical density and register differentiation.
    In G. E. Perren & J. L. M. Trim (Eds.), Applications of
    linguistics (pp. 443–452). Cambridge University Press.

================================================================
[Koniec notatki — checkLit, projekt inżynierski, luty 2026]
[Etapy: 7 | Testy: 86/88 passed | Korpus: 80 tekstów | AUC: 0.90]
================================================================
