================================================================
CHECKLIT – PODSUMOWANIE ETAP 3: EWALUACJA I KALIBRACJA MODELU
================================================================
Data: luty 2026
Korpus: 25 human + 25 AI = 50 tekstów

----------------------------------------------------------------
WYNIKI EWALUACJI (evaluate.py, corpus.csv n=50)
----------------------------------------------------------------

  AUC-ROC:              0.7936
  Accuracy:             0.80  (80%)
  Precision:            0.7273
  Recall:               0.96
  F1 Score:             0.8276
  Optymalny próg:       38.45 (metoda Youdenа na krzywej ROC)

  Macierz pomyłek:
                   Pred: Human   Pred: AI
    True: Human       16            9     ← 9 fałszywych alarmów
    True: AI           1           24     ← 1 przeoczony

  Pliki wyjściowe: roc_curve.png, evaluation_results.csv,
                   evaluation_summary.json

----------------------------------------------------------------
INTERPRETACJA WYNIKÓW
----------------------------------------------------------------

  ✅ Recall = 0.96
     System wykrywa 96% tekstów AI (przeoczył tylko 1 z 25).
     Dla systemu detekcji wysoki recall to priorytet — lepiej
     fałszywy alarm niż przeoczony tekst AI.

  ⚠️ Precision = 0.73
     27% fałszywych alarmów — część tekstów ludzkich błędnie
     flagowana jako AI.

  ANALIZA BŁĘDÓW — fałszywe alarmy (9 tekstów ludzkich → AI):
    Prus (Wokulski w pociągu)          perplexity = 28.26
    Reymont (Chłopi siedzieli)         perplexity = 29.13
    Proza współczesna (styl gładki)    perplexity = 25–36

    Wzorzec: teksty o spokojnym, narracyjnym toku zdań
    i niezbyt rozbudowanym słownictwie mają perplexity
    podobną do tekstów AI. To znane ograniczenie metody
    perplexity-based — opisane w literaturze naukowej
    (Gehrmann et al., 2019; Ippolito et al., 2019).

  WNIOSEK DO PRACY:
    AUC = 0.79 przekracza próg akceptowalności (0.75).
    Wynik jest dobry jak na 1-metryczny system bez fine-tuningu
    i bez danych treningowych specyficznych dla literatury.
    Sposób na poprawę precision: fine-tuning GPT-2 na tekstach
    literackich (poza zakresem tej pracy — temat dla przyszłych
    badań).

----------------------------------------------------------------
ZAKTUALIZOWANE PROGI W ai_detector.py
----------------------------------------------------------------

  Poprzednie (zgadnięte):   AI < 25,  Human > 40
  Nowe (wyznaczone ROC):    AI < 30,  Human > 38.45

  Strefa szara: 30.0 – 38.45
    → confidence = "Niska (strefa szara – wynik niepewny)"
    → informuje użytkownika że wynik jest niejednoznaczny

----------------------------------------------------------------
SKŁAD KORPUSU EWALUACYJNEGO
----------------------------------------------------------------

  HUMAN (25):
    Wolne Lektury — Sienkiewicz (Quo Vadis, 1896)
    Wolne Lektury — Prus (Lalka t.1 i t.2, 1890)
    Wolne Lektury — Reymont (Chłopi, 1904)
    Wolne Lektury — Żeromski (Ludzie bezdomni t.1 i t.2, 1900)
    Proza współczesna (oryginalna, styl ludzki) — 15 tekstów

  AI (25):
    Wygenerowane przez Claude (Anthropic) — 25 tekstów
    Różne style: narracja, opis, dialog, wspomnienie

----------------------------------------------------------------
NOWE FUNKCJONALNOŚCI (wdrożone w etapie 3)
----------------------------------------------------------------

  backend/app/services/file_parser.py
    → Ekstrakcja tekstu z .txt, .pdf, .docx
    → Czyszczenie artefaktów PDF (łamanie wierszy mid-word)

  backend/app/routers/analysis.py
    → Nowy endpoint: POST /api/analyze-file (multipart/form-data)
    → Limit: 10MB (~10+ stron A4)
    → Wspólny pipeline z /analyze (refaktoryzacja run_analysis_pipeline)

  frontend/src/pages/Analyze.jsx
    → Tryb "Wklej tekst" / "Wgraj plik" (toggle)
    → Drag & drop + click-to-upload
    → Podgląd wybranego pliku z rozmiarem
    → Walidacja formatu i rozmiaru po stronie klienta

  backend/ai_detector.py
    → Progi skalibrowane matematycznie (ROC/Youden)
    → Strefa szara 30–38.45 z dedykowaną etykietą
    → Komentarze z metrykami ewaluacji

  start.ps1
    → Skrypt PowerShell uruchamiający backend + frontend
    → Otwiera dwa okna terminala automatycznie

----------------------------------------------------------------
NOWE PACZKI (dodać do requirements.txt)
----------------------------------------------------------------

  pypdf>=3.0.0
  python-docx>=1.1.0
  scikit-learn       (evaluate.py)
  matplotlib         (evaluate.py — wykresy ROC)
  pandas             (evaluate.py)

  Instalacja:
    pip install pypdf python-docx scikit-learn matplotlib pandas

----------------------------------------------------------------
STATUS PROJEKTU
----------------------------------------------------------------

  ✅ [1]  Setup
  ✅ [2]  Stylometria + NLP (LIX)
  ✅ [3]  Detekcja AI — skalibrowana, przetestowana, AUC=0.79
  ✅ [4]  API endpoints
  ✅ [5]  Upload pliku (.txt/.pdf/.docx)
  ✅ [6]  Ewaluacja (ROC, metryki, wykres)
  ⚠️ [7]  Frontend — kod gotowy, NIE testowany end-to-end z API
  ❌ [8]  Pytest — NIE uruchomione
  ❌ [9]  Eksport raportu (JSON/PDF)

----------------------------------------------------------------
CO DALEJ (ETAP 4)
----------------------------------------------------------------

  → Test frontendu end-to-end (localhost:5173)
  → Uruchomienie pytest
  → Implementacja eksportu raportu
  → Pisanie rozdziałów pracy (metodologia, testy, wyniki)

================================================================
