================================================================
CHECKLIT – PODSUMOWANIE ETAP 6: EWALUACJA LIVE + REKALIBRACJA v3
================================================================
Data: luty 2026

----------------------------------------------------------------
CO ZROBILIŚMY W ETAPIE 6
----------------------------------------------------------------

1. Zbudowano skrypt automatycznej ewaluacji live (evaluate_live.py)
2. Przeprowadzono test na 30 tekstach (15 AI + 15 human)
3. Wykryto problem z kalibracją progów
4. Rozszerzono korpus do 80 tekstów (corpus_full.csv)
5. Uruchomiono rekalibrację → nowe progi v3
6. Zaktualizowano interfejs Results.jsx i Compare.jsx

----------------------------------------------------------------
PROBLEM: PROGI POZA ZAKRESEM
----------------------------------------------------------------

Skrypt evaluate_live.py wysłał 30 tekstów do API i zebrał wyniki.
Wszystkie 30 tekstów dostało ai_probability = 0.03 (Human-written).

Przyczyna:
  Progi w ai_detector.py (v2): AI < 28.89, Human > 37.04
  Perplexity 30 nowych tekstów: zakres 99–677

  Wszystkie teksty leżały ~200+ jednostek powyżej progu "human".
  Model zawsze odpowiadał "human" z pewnością 97%.

Dlaczego tak duże wartości perplexity?
  Korpus kalibracyjny v2 zawierał AI pisane formulaicznie (ppx ~14–35).
  Nowe teksty AI były pisane starannym stylem literackim (ppx 99–271).
  GPT-2 jest tak samo "zaskoczony" staranną prozą AI jak prozą ludzką.

----------------------------------------------------------------
ANALIZA ROZKŁADU PERPLEXITY (30 tekstów)
----------------------------------------------------------------

  Klasa    Śr.ppx   Mediana   Min     Max
  ───────────────────────────────────────
  AI       192.7    184.8     99.6    270.9
  Human    310.4    258.9     171.0   677.0

  Nakładanie zakresów: 171–271 (duże!)
  Najlepsze osiągalne accuracy przy 1 progu: ~73%

  Wniosek: teksty AI pisane stylem literackim są znacznie trudniejsze
  do wykrycia metodą perplexity niż formulaiczne generacje AI.

----------------------------------------------------------------
REKALIBRACJA v3: KORPUS 80 TEKSTÓW
----------------------------------------------------------------

  Dodano 30 nowych tekstów do corpus.csv przez make_corpus.py
  Uruchomiono evaluate.py na corpus_full.csv (80 tekstów)

  WYNIKI EWALUACJI v3:

  AUC-ROC:              0.9029   (poprzednio v2: 0.9376)
  Accuracy:             0.825    (poprzednio: 0.90)
  Precision:            0.7609   (poprzednio: 0.8846)
  Recall:               0.9211   (poprzednio: 0.9200)
  F1:                   0.8333   (poprzednio: 0.9020)
  Optymalny próg:       41.0623  (poprzednio: 37.043)

  Macierz pomyłek (n=80):
                   Pred: Human   Pred: AI
    True: Human        31           11
    True: AI            3           35

  Fałszywe alarmy (11 tekstów ludzkich → AI):
    Wokulski w pociągu (Prus)         ppx = 28.26
    Faraon t.1 Wstęp (Prus)           ppx = 38.27
    Soroka i Kmicic (Sienkiewicz)      ppx = 36.46
    Boryna na miedzy (Reymont)         ppx = 29.19
    Ziemia obiecana — Bauer (Reymont)  ppx = 37.06
    Ksiądz Robak (Mickiewicz)          ppx = 37.53
    Dulska (Zapolska)                  ppx = 27.29
    Nad Niemnem — Justyna (Orzeszkowa) ppx = 29.21
    W pustyni i w puszczy (Sienkiewicz)ppx = 39.19
    Babka Agnieszka (proza ludowa)     ppx = 35.10
    Nad Niemnem — Tadeusz (Orzeszkowa) ppx = 34.50

  Przeoczenia AI (3 teksty AI → Human):
    Claude — pisarka przy biurku (Anna)       ppx = 45.33
    Claude — kamienica przy Złotej            ppx = 41.06
    Claude — opis rzeczki                     ppx = 40.91
    (+ 1 z v2: nan ppx = 47.35)

----------------------------------------------------------------
ZAKTUALIZOWANE PROGI W ai_detector.py (v3)
----------------------------------------------------------------

  PERPLEXITY_AI_THRESHOLD    = 32.03
  PERPLEXITY_HUMAN_THRESHOLD = 41.0623

  Strefa szara: 32.03 – 41.06

----------------------------------------------------------------
INTERPRETACJA WYNIKÓW — WNIOSEK BADAWCZY
----------------------------------------------------------------

  Porównanie dwóch scenariuszy testowych:

  Scenariusz          n    AUC    Accuracy  Precision  Recall
  ──────────────────────────────────────────────────────────
  v2 — formulaiczny  50   0.94    90%        88%       92%
  v3 — literacki     80   0.90    82.5%      76%       92%

  Recall jest stabilny (92%) — system prawie zawsze wykrywa
  formulaiczne AI i większość literackiego AI.

  Precision spada (88% → 76%) — więcej fałszywych alarmów
  przy tekstach ludzkich o prostej, regularnej narracji.

  Wzorzec błędów (rozszerzony):
    Ludzkie teksty z niską perplexity (27–40):
      - Narracja psychologiczna z krótkimi zdaniami (Dulska, Orzeszkowa)
      - Proza epicka z prostym tokiem (Boryna, Wokulski)
      - Dialog narracyjny (Zapolska)

    Teksty AI z wysoką perplexity (40–47):
      - AI z metaforycznym, poetyckim stylem (opis rzeczki)
      - AI z nieregularną emocjonalną składnią (kamienica, pisarka)

  Wniosek metodologiczny:
    Perplexity mierzy "regularność" stylu, nie "człowieczeństwo".
    Metoda zawodzi gdy ludzki autor pisze regularnie lub AI pisze
    nieregularnie. Jest to znane ograniczenie opisane w literaturze
    (Gehrmann et al., 2019; Ippolito et al., 2019).

    Dla pracy inżynierskiej: AUC = 0.90 na trudniejszym korpusie
    to nadal wynik powyżej progu akceptowalności (0.75) i potwierdza
    użyteczność systemu przy odpowiednim informowaniu o ograniczeniach.

----------------------------------------------------------------
NOWE PLIKI Z ETAPU 6
----------------------------------------------------------------

  evaluate_live.py       → automatyczna ewaluacja przez API
  make_corpus.py         → łączenie starych i nowych danych
  corpus_full.csv        → 80 tekstów (38 AI + 42 human)
  perplexity_analysis.py → analiza rozkładu, sugestia progów
  Results.jsx            → zaktualizowany UI (skale, ostrzeżenia)
  Compare.jsx            → zaktualizowany UI (skala podobieństwa)

  Wygenerowane przez evaluate.py (n=80):
    roc_curve.png              → nowy wykres ROC (AUC=0.90)
    evaluation_results.csv     → wyniki per tekst
    evaluation_summary.json    → metryki v3

----------------------------------------------------------------
STATUS PROJEKTU
----------------------------------------------------------------

  ✅ Setup środowiska
  ✅ Stylometria + NLP (MATTR, LIX, gęstość leksykalna, entropia)
  ✅ Detekcja AI — model Polish GPT-2
  ✅ Kalibracja v1 (n=50, AUC=0.79, korpus zanieczyszczony)
  ✅ Rekalibracja v2 (n=50, AUC=0.94, czysty korpus Wolne Lektury)
  ✅ Rekalibracja v3 (n=80, AUC=0.90, AI literacki + Wolne Lektury)
  ✅ Pytest — 86/88 passed (97.7%)
  ✅ API FastAPI + endpointy
  ✅ Upload pliku (.txt/.pdf/.docx)
  ✅ Eksport raportu (JSON + PDF print)
  ✅ Ewaluacja live — skrypt automatyczny
  ✅ Frontend React — skale, ostrzeżenia, ograniczenia
  ⚠️ Frontend — nie testowany end-to-end po aktualizacji Results.jsx
  ❌ Testy frontendu (Vitest)
  ❌ Deployment (Docker)

----------------------------------------------------------------
BIBLIOGRAFIA DO DODANIA (materiał z etapu 6)
----------------------------------------------------------------

  Gehrmann, S., Strobelt, H., & Rush, A. M. (2019).
    GLTR: Statistical Detection and Visualization of Generated Text.
    ACL 2019. https://arxiv.org/abs/1906.04043

  Ippolito, D., Duckworth, D., Callison-Burch, C., & Eck, D. (2019).
    Automatic Detection of Generated Text is Easiest when Humans are
    Fooled. arXiv:1911.00650.

----------------------------------------------------------------
CO DALEJ (ETAP 7)
----------------------------------------------------------------

  Priorytety:
  1. Test end-to-end frontendu po aktualizacji komponentów
  2. Pisanie pracy:
     - Rozdział 4.3: implementacja detekcji AI (metodologia)
     - Rozdział 5.1: ewaluacja v1/v2/v3 (tabela porównawcza)
     - Rozdział 5.2: analiza błędów klasyfikacji
     - Rozdział 6: wnioski i ograniczenia metody
  3. Opcjonalnie: test z prawdziwymi tekstami studentów
     (weryfikacja czy system działa na innym gatunku niż literatura)

  Materiały gotowe do pracy:
  → Tabela porównawcza v1/v2/v3 (powyżej)
  → Wykres ROC (roc_curve.png, AUC=0.90)
  → Lista błędnych klasyfikacji z przyczynami
  → evaluation_summary.json → tabela do wklejenia

================================================================
